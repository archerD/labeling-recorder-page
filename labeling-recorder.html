<!DOCTYPE html>
<html>
<head>
  <title>The Label Recorder Method - AMISTAD Lab</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <link rel="stylesheet" type="text/css" href="projects.css">
  <link rel="stylesheet" type="text/css" href="labeling-recorder.css">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <!-- Google Analytics setup -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83215098-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- MathJax Setup -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
<div div style="margin: auto;" class="main-page-box">
  <!-- AMISTAD x NSF x HMC -->
  <img src="./images/amistad-logo.png" alt="AMISTAD Logo" class="header-logo" />

  <h1>The Label Recorder Method</h1>
  <h2 class="subtitle">2021</h2>
  <h3>Overview</h3>
  <p>
    Imagine you are on a routine grocery shopping trip and plan to buy some bananas. You know that the store carries both good and bad bananas which you must search through. There are multiple ways you can go about your search. <br><br>

    You could: <br>
    1) randomly pick any ten bananas available on the shelf (unbiased search) <br>
    2) Only pick those bananas that are neither unripe nor overripe (biased search) <br><br>

    We mathematically prove that for a machine, the proportion of good bananas retrieved in its biased search is greater than the same proportion in an unbiased search; it was given prior knowledge about tasty bananas. <br><br>

    We further prove that an algorithm cannot be simultaneously biased towards all types of bananas (unripe, overripe, perfect).
  </p>

  <!-- Definitions -->
  <h3>Key Definitions</h3>
  <h4>Bias</h4>
  <p>
    Statistical bias is when the expected value of a parameter differs from its true value.<br><br>

    We define algorithmic bias to be a measure of how different the performance of an algorithm is with respect to uniform random sampling. <br><br>

    Given a: <br>
      &bull; \(k\)-sized fixed target \(t\)<br>
      &bull; distribution over information resources \(\mathcal{D}\) <br>
      &bull; search strategy \(\overline{P}_F\) (i.e., inductive orientation relative to \(F\)) <br>
      &bull; performance of uniform random sampling \(p\) <br>

    $$\text{Bias}(\mathcal{D},t) = \mathbb{E}_{\mathcal{D}}\left[ t^{\top} \overline{P}_F \right] - p$$

    <span style="font-weight:400;">Note:</span> algorithmic bias has very little to do with unethical human biases (racial, xenophobic, etc.). Our version of algorithmic bias is <span style="font-style:italic;">strictly</span>
    defined in the domain of mathematics, and human examples of it include picking the simplest solution from a set, and choosing a particular
    type of learning algorithm to solve a problem.
  </p>
  <h4>Target Divergence</h4>
  <p>
  What does algorithmic bias look like? Target divergence is a geometric interpretation of it which measures the angle between the target vector \(t\) and the averaged \(|\Omega|\)-length simplex vector \(\overline{P}_f\) using cosine similarity.
  </p>
  <div>
    <div style="margin-bottom: 0%;">
      <div style="display:inline-block;">
        <img src="./images/target-divergence-1.png" alt="Target Divergence 1" class="target-divergence-diagram">
      </div>
      <div style="display:inline-block;">
        <img src="./images/target-divergence-2.png" alt="Target Divergence 2" class="target-divergence-diagram">
      </div>
    </div>
    <div>
      <div style="display:inline-block; margin-top: 4%;">
        <img src="./images/target-divergence-3.png" alt="Target Divergence 3" class="target-divergence-diagram">
      </div>
    </div>
  </div>

  <!-- Important Theorems -->
  <h3>Results</h3>
  <h4>Improbability of Favorable Information Resources</h4>
  <p>
    Let \(F\) be a random variable such that \(F\) ~ \(\mathcal{D}\) and let \(q(t,F)\) be the expected per-query probability of success for algorithm \(\mathcal{A}\) on search problem \((\Omega, t, F)\). Then for any \(q_{min}\) \(\in\) [0,1],

    $$\mathbb{P}(q(t,F) \geq q_{min}) \leq \frac{p + \text{Bias}(\mathcal{D}, t)}{q_{min}}$$

    Using the per-query probability of success from <a href="https://arxiv.org/pdf/1609.08913.pdf">The Famine of Forte</a>, we bound the probability that an algorithm is successful, this time in terms of its bias.
    This shows that one is unlikely to encounter highly favorable information resources when sampling under \(\mathcal{D}\) unless the algorithm is tuned to that particular target \(t\).
  </p>
  <h4>Conservation of Bias over Distributions</h4>
  <p>
    Let \(F\) be a random variable such that \(F\) ~ \(\mathcal{D}\) and let \(q(t,F)\) be the expected per-query probability of success for algorithm \(\mathcal{A}\) on search problem \((\Omega, t, F)\). Then for any \(q_{min}\) \(\in\) [0,1],

    $$\sum_{t \in \tau_{k}} \int_{\mathcal{P}} \text{Bias}(\mathcal{D},t) d\mathcal{D} = 0$$

    Similar to conservation laws for energy and mass in physics, we show that bias is a conserved quantity. <br><br>
    An algorithm can have positive bias on a particular target, but this implies it also has negative bias on some other targets in the search space.
    We show that the total bias an algorithm has is 0, thus bias encodes trade-offs and we must choose which targets we want to be biased towards. <br><br>

    This further means that an algorithm cannot be biased towards all possible targets such that it would perform well on all problems.
  </p>
  <h4>Futility of Bias-Free Search</h4>
  <p>
    If \( \text{Bias}(\mathcal{D},t) = 0 \) then,

    $$ \mathbb{P}(\omega \in t; \mathcal{A}) = p $$

    Our bias quantity essentially captures how much more likely an algorithm is to find an acceptable model to fit some data than by picking a model at random.
    Thus when an algorithm has no bias, the probability it will find an acceptable model is equal to the probability of it uniformly randomly sampling from a set of possible models.
  </p>

  <!-- Authors and Headshots -->
  <!--
  <h3>Authors</h3>
  <div>
    <div style="margin-bottom: 3%;">
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/jonathan-hayase-5ab849128/">
          <img src="../images/jonathan.jpg" alt="Jonathan Hayase" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/jonathan-hayase-5ab849128/" class="text-link">
          <p style="text-align:center; margin: 0;">Jonathan Hayase</p>
        </a>
      </div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/julius-lauw-882652139/">
          <img src="../images/julius.jpg" alt="Julius Lauw" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/julius-lauw-882652139/" class="text-link">
          <p style="text-align:center; margin: 0;">Julius Lauw</p>
        </a>
      </div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/dominique-macias/">
          <img src="../images/dom.jpg" alt="Dominique Macias" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/dominique-macias/" class="text-link">
          <p style="text-align:center; margin: 0;">Dominique Macias</p>
        </a>
      </div>
    </div>
    <div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/akshay-trikha">
          <img src="../images/akshay.jpg" alt="Akshay Trikha" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/akshay-trikha" class="text-link">
          <p style="text-align:center; margin: 0;">Akshay Trikha</p>
        </a>
      </div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/julia-vendemiatti-645a5613a/">
          <img src="../images/amistad/julia.jpg" alt="Julia Vendemiatti" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/julia-vendemiatti-645a5613a/" class="text-link">
          <p style="text-align:center; margin: 0;">Julia Vendemiatti</p>
        </a>
      </div>
      <div style="display:inline-block;">
        <a href="mailto:gmontanez@g.hmc.edu">
          <img src="../images/amistad/prof-george.png" alt="George Monta&ntilde;ez" class="headshot">
        </a>
        <a href="mailto:gmontanez@g.hmc.edu" class="text-link">
          <p style="text-align:center; margin: 0;">George Monta&ntilde;ez</p>
        </a>
      </div>
    </div>
  </div>
  -->

  <!-- Thank you(s) -->
  <h3>Acknowledgements</h3>
  <p>
    <!-- We'd like to thank our loving families and supportive friends, and the Walter Bradley Center for Natural and Artificial Intelligence for their generous support of this research. -->
  <p>
  <!-- Citations -->
  <h3>Further Reading</h3>
  <ul class="references">
      <li>Monta&ntilde;ez, G.D., Hayase, J., Lauw, J., Macias, D., Trikha, A., Vendemiatti, J. :The Futility of Bias-Free Learning and Search. 2nd Australasian Joint Conference on Artificial Intelligence (AI 2019), 2019. 277&mdash;288</li>
      <li>Monta&ntilde;ez, G.D. :The Famine of Forte: Few Search Problems Greatly Favor Your Algorithm.  In: 2017 IEEE International Conference on Systems, Man, and Cybernetics(SMC). pp. 477&mdash;482.</li>
      <li>Wolpert, D.H., Macready, W.G.: No free lunch theorems for optimization. Trans.Evol. Comp1(1), 67&mdash;82 (Apr 1997)</li>
      <li>Mitchell, T.D.: The need for biases in learning generalizations. In: Rutgers University: CBM-TR-117 (1980)</li>
  </ul>
  <div class="bottom-nav">
    <a href="projects.html">Back to Projects</a>
  </div>
</body>
</html>
